{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIN538_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQa3wVJu-4Lk"
      },
      "source": [
        "# Language Models: *n*-grams and PCFGs\r\n",
        "### Final Project for LIN 538: Statistics for Linguists (Fall 2020)\r\n",
        "*Derek Andersen and Joanne Chau*  \r\n",
        "*Stony Brook University*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxJjcgvH_T9E"
      },
      "source": [
        "In this project we will train and compare the results of two different types of language models: a **trigram** model and a **PCFG** (probabilistic context-free grammar). In both cases, we will use **perplexity** as a metric for evaluation, and we will compare the two models according to their perplexity values using the Wall Street Journal corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxefXBzJAVm9"
      },
      "source": [
        "## Trigram Model\r\n",
        "A trigram model predicts the probability, *p*, of a word, *w*, occurring next in a sentence, given a history, *h*, of *n – 1* words (in this case, *h = 2*). So our trigram model will be able to predict *p(w | h)*, or more specifically, *p(w3 | w1 w2)*.  \r\n",
        "  \r\n",
        "First, we import `nltk` and other necessary libaries. Then we define the functions we'll need: `trigram_model` to train our model, `generate_sentence` to demo sentence generation with our model, and `perplexity` to calculate the perplexity of a model according to a test sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQTw-OrC-gVT"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import trigrams\r\n",
        "from collections import Counter, defaultdict\r\n",
        "import random\r\n",
        "from pathlib import Path\r\n",
        "import os\r\n",
        "\r\n",
        "# Path to wsj corpus\r\n",
        "corpus_path = Path(\"C:/Users/Derek/Documents/wsj_corpus\")\r\n",
        "\r\n",
        "def trigram_model(corpus_path):\r\n",
        "    \"\"\"Builds a trigram model trained on a training corpus.\"\"\"\r\n",
        "    # Smoothing of 0.01 to handle unattested words in test data\r\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0.01))\r\n",
        "    # Training set of 80% of the Wall Street Journal corpus (first 1963 files)\r\n",
        "    for file in os.listdir(corpus_path)[:1964]:\r\n",
        "        with open(corpus_path / file, 'r') as current:\r\n",
        "            sents = current.readlines()\r\n",
        "            for sentence in sents:\r\n",
        "                if ('.START' in sentence) or (sentence == '\\n'):\r\n",
        "                    continue\r\n",
        "                else:\r\n",
        "                    sentence = sentence.split()\r\n",
        "                for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\r\n",
        "                    model[(w1, w2)][w3] += 1\r\n",
        "\r\n",
        "    # Transform the counts into probabilities\r\n",
        "    for w1_w2 in model:\r\n",
        "        total_count = float(sum(model[w1_w2].values()))\r\n",
        "        for w3 in model[w1_w2]:\r\n",
        "            model[w1_w2][w3] /= total_count\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "def generate_sentence(model):\r\n",
        "    \"\"\"Generates a sentence according to a trigram model.\"\"\"\r\n",
        "    text = [None, None]\r\n",
        "    sentence_finished = False\r\n",
        "\r\n",
        "    while not sentence_finished:\r\n",
        "        r = random.random()\r\n",
        "        accumulator = .0\r\n",
        "        for word in model[tuple(text[-2:])].keys():\r\n",
        "            accumulator += model[tuple(text[-2:])][word]\r\n",
        "            if accumulator >= r:\r\n",
        "                text.append(word)\r\n",
        "                break\r\n",
        "        if text[-2:] == [None, None]:\r\n",
        "            sentence_finished = True\r\n",
        "    print(' '.join([t for t in text if t]))\r\n",
        "\r\n",
        "def perplexity(test_sent, model):\r\n",
        "    \"\"\"Computes the perplexity of a trigram model on a test sentence.\"\"\"\r\n",
        "    test_sent = test_sent.split()\r\n",
        "    perplexity = 1\r\n",
        "    N = 0\r\n",
        "    for w1, w2, w3 in trigrams(test_sent, pad_right=True, pad_left=True):\r\n",
        "        N += 1\r\n",
        "        perplexity = perplexity * (1/model[(w1, w2)][w3])\r\n",
        "    perplexity = pow(perplexity, 1/float(N))\r\n",
        "    return perplexity\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBVw0TqYZoAV"
      },
      "source": [
        "### Training the model\r\n",
        "First, we train our model using 80% of the Wall Street Journal corpus and save it as `model`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPpgV4zeaQoj"
      },
      "source": [
        "# Create a trigram model according to wsj corpus\r\n",
        "model = trigram_model(corpus_path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_W7l1oBcYIU"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcStET2faXru"
      },
      "source": [
        "Now, as a test, let's look at the probability that a sentence will start with 'The' according to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4c9hCoSanSP",
        "outputId": "a6af9632-b059-43c6-c417-4263b4b8c716"
      },
      "source": [
        "# Print the probability that a sentence will start with 'The'\r\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16637225876894499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtALjzJxa1iF"
      },
      "source": [
        "To illustrate how the model will behave when it's asked to predict a word unattested in the training set, let's run this same test with a made-up  word following an *attested* history: 'political concerns'. We can see that the model predicts 0.01, which is treated as 0. This 0.01 is a result of the smoothing we applied when training our model, so that it can handle unattested words in testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoIbgmLwbQl9",
        "outputId": "2eb2b1be-a7d6-470a-90de-49555575a5b8"
      },
      "source": [
        "print(model['political', 'concerns']['madeupword'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g4e2SgWb4yi"
      },
      "source": [
        "Now we can use our model to generate a new sentence, to demo its nativeness. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnWNy7F9cM9e",
        "outputId": "7a0fa5fe-52ea-401c-86b9-aac5d9a83e9a"
      },
      "source": [
        "generate_sentence(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Consider, for instance, is forecasting growth in the oil-patch state of Alagoas.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2s5gdVccbwn"
      },
      "source": [
        "### Perplexity\r\n",
        "The metric we're using for model comparison is **perplexity**. Perplexity is a measure of how good, or in the case of language, how native a model is. A lower perplexity is a correlate of higher nativeness, and ideally a model trained on English data should be able to recognize English sentences well, and thus score lower on the perplexity spectrum.  \r\n",
        "  \r\n",
        "The perplexity, *PP*, of a sentence, *s*, can be calculated with the following:  \r\n",
        "  \r\n",
        "*PP(s) = p(w1, ..., wn)^(–1/n)*  \r\n",
        "  \r\n",
        "We will use a test set of 20% of the Wall Street Journal corpus to evaluate the perplexity of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zwn1plPZYAl",
        "outputId": "80d613dd-2843-401d-fce4-fcd9dbd84dd0"
      },
      "source": [
        "# Construct a test set of 20% of the Wall Street Journal corpus (files 1964 - 2454)\r\n",
        "testset = []\r\n",
        "for file in os.listdir(corpus_path)[1964:2455]:\r\n",
        "    with open(corpus_path / file, 'r') as current:\r\n",
        "        sents = current.readlines()\r\n",
        "        for sentence in sents:\r\n",
        "            if ('.START' in sentence) or (sentence == '\\n'):\r\n",
        "                continue\r\n",
        "            else:\r\n",
        "                testset.append(sentence)\r\n",
        "\r\n",
        "# Calculate the perplexity of the model with the entire test set\r\n",
        "PP = 0\r\n",
        "perplexities = []\r\n",
        "i = 0\r\n",
        "for sentence in testset:\r\n",
        "    p = perplexity(sentence, model)\r\n",
        "    # ignore infinity cases\r\n",
        "    if p == float(\"inf\"):\r\n",
        "        continue\r\n",
        "    i += 1\r\n",
        "    PP += p\r\n",
        "# average of perplexities\r\n",
        "PP = PP / i\r\n",
        "\r\n",
        "\r\n",
        "print('Model perplexity on test set:', PP)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model perplexity on test set: 55.434798009501684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2MOUtE0e_sZ"
      },
      "source": [
        "For reference, let's test the same model's perplexity instead using a Jabberwocky sentence (a sentence with several made-up words). We can see that the perplexity is much higher, as we get a value of 100. This is what we would expect considering the lower perplexity value with the English test set of unattested sentences we used earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixub9XZwfan5",
        "outputId": "e484ce9d-f184-47ef-95da-0f137687559d"
      },
      "source": [
        "jabberwocky = \"Twas brilig, and the slithy toves Did gyre and gimble in the wabe; All mimsy were the borogoves, And the mome raths outgrabe.\"\r\n",
        "print(perplexity(jabberwocky, model))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.00000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuBvfWTNmNKg"
      },
      "source": [
        "## PCFG Model \n",
        "A PCFG model takes a set of training sentences and examines all context free grammar rules. It then saves it as a full list of all productions. \n",
        "\n",
        "In order to have an adequate number of trees to train and test the model, we  saved all `.mrg` files from the Wall Street Journal treebank in the `nltk_data` directory for easier access, since there are already built-in functions in `nltk` for dealing with treebanks.\n",
        "\n",
        "Instead of the 80/20 split we did with the trigram model, the PCFG model will run on a small portion of the training set (60 files) as it takes too long for the full training set to get all the PCFG rules. We will save our model in `grammar`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94JYz3WmqqkG"
      },
      "source": [
        "import nltk\n",
        "from nltk import Tree, PCFG, treetransforms, induce_pcfg, Nonterminal\n",
        "from nltk.corpus import treebank\n",
        "from nltk.parse import pchart, ViterbiParser\n",
        "\n",
        "def make_PCFG_grammar():\n",
        "\n",
        "  '''\n",
        "  Trains a PCFG grammar on the WSJ treebank. \n",
        "  '''\n",
        "\n",
        "  # Save a list of all produced PCFG rules given the test data\n",
        "  PCFG_rules = []\n",
        "\n",
        "  # We'll utilize the Wall Street Journal corpus\n",
        "  for item in treebank.fileids()[:61]:\n",
        "      # We want to first get rid of all non-binary branchings of the tree\n",
        "      for tree in treebank.parsed_sents(item):\n",
        "          tree.collapse_unary(collapsePOS = False)\n",
        "          tree.chomsky_normal_form(horzMarkov = 2)\n",
        "          PCFG_rules += tree.productions()\n",
        "\n",
        "  # Induce the PCFG grammar\n",
        "  S = Nonterminal('S')\n",
        "  PCFG_grammar = induce_pcfg(S, PCFG_rules)\n",
        "\n",
        "  return PCFG_grammar\n",
        "\n",
        "\n",
        "# save our model\n",
        "grammar = make_PCFG_grammar()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka9pSca_YPo4"
      },
      "source": [
        "### Test set \n",
        "\n",
        "With our PCFG grammar, we can use the built-in `ViterbiParser` function available in `nltk` to build a CKY parser applicable to our PCFG rules. Using this, we can run test trees through it to find their most probable parses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWdE8HqkYceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95092ced-7d85-40a0-9777-b9760c5ae52e"
      },
      "source": [
        "def CKY_parser(PCFG_grammar):\n",
        "\n",
        "    '''\n",
        "    Given the PCFG grammar, we use the built in CKY praser function\n",
        "    to get a sentence's most probable parse\n",
        "    '''\n",
        "\n",
        "    # Utilize the ViertabiParser given the PCFG grammar induction rules\n",
        "    parser = ViterbiParser(PCFG_grammar)\n",
        "\n",
        "    # Sample file parse for reference \n",
        "    sentences = treebank.parsed_sents('wsj_0001.mrg')\n",
        "\n",
        "    skipped_sentences = 0\n",
        "\n",
        "    # A for loop to get all possible trees within the files \n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.leaves()\n",
        "        \n",
        "        # To speed up the code, we'll check with the grammar first \n",
        "        # And ensure that all words are accounted for\n",
        "        # If it is not accounted for, skip the sentence \n",
        "        # And increment skipped_sentences\n",
        "        try:\n",
        "            PCFG_grammar.check_coverage(sentence)\n",
        "            \n",
        "            # Print the final parse of the sentence \n",
        "            for parse in parser.parse(sentence):\n",
        "                print(parse)\n",
        "        except:\n",
        "            skipped_sentences += 1\n",
        "            continue\n",
        "\n",
        "    print(\"Total skipped sentences:\", skipped_sentences)\n",
        "\n",
        "\n",
        "# demo the parser\n",
        "CKY_parser(grammar)  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP-SBJ-27\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (NP-SBJ-27|<,-ADJP>\n",
            "      (, ,)\n",
            "      (NP-SBJ-27|<ADJP-,>\n",
            "        (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "        (, ,))))\n",
            "  (S|<VP-.>\n",
            "    (VP\n",
            "      (MD will)\n",
            "      (VP\n",
            "        (VB join)\n",
            "        (VP|<NP-PP-LOC>\n",
            "          (NP (DT the) (NN board))\n",
            "          (VP|<PP-LOC-NP-TMP>\n",
            "            (PP-LOC\n",
            "              (IN as)\n",
            "              (NP\n",
            "                (DT a)\n",
            "                (NP|<JJ-NN>\n",
            "                  (JJ nonexecutive)\n",
            "                  (NN director))))\n",
            "            (NP-TMP (NNP Nov.) (CD 29))))))\n",
            "    (. .))) (p=2.39483e-50)\n",
            "(S\n",
            "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
            "  (S|<VP-.>\n",
            "    (VP\n",
            "      (VBZ is)\n",
            "      (NP-PRD\n",
            "        (NP (NN chairman))\n",
            "        (PP\n",
            "          (IN of)\n",
            "          (NP\n",
            "            (NP (NNP Elsevier) (NNP N.V.))\n",
            "            (NP|<,-NP>\n",
            "              (, ,)\n",
            "              (NP\n",
            "                (DT the)\n",
            "                (NP|<NNP-VBG>\n",
            "                  (NNP Dutch)\n",
            "                  (NP|<VBG-NN> (VBG publishing) (NN group)))))))))\n",
            "    (. .))) (p=2.27472e-37)\n",
            "Total skipped sentences: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywV-yon3S6Ox"
      },
      "source": [
        "We can see two output parses above, along with their probabilities (*p* values). These two sentences are present in the model's training set and thus all of their vocabulary is attested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ6Qy6-iY7sb"
      },
      "source": [
        "### Perplexity\n",
        "\n",
        "In order to calculate the perplexity of the PCFG model, run all testing set and then save all probabilities for the best parse. \n",
        "\n",
        "We will save the probabilities into a list of probabilities for reference. Then we utilize that to calculate the perplexity of the model. \n",
        "\n",
        "#### Issues with testing\n",
        "Unfortunately, with our training set, the amount of time it takes to test on the model increases exponentially and it is not feasible to run all of the tests. In our case, we used 2 files from the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuqTocgSY_q5"
      },
      "source": [
        "import re\n",
        "\n",
        "def all_parse_probabilities(PCFG_grammar):\n",
        "  \n",
        "  '''\n",
        "  Given the PCFG grammar, we utilize the CKY parser to get \n",
        "  the test set's parse probabilties. \n",
        "  '''\n",
        "\n",
        "  parser = ViterbiParser(PCFG_grammar)\n",
        "  # Make a list to save all extracted parse probabilities \n",
        "  all_p = []\n",
        "\n",
        "  # 2 test files\n",
        "  for item in treebank.fileids()[1964:1966]:\n",
        "      trees = treebank.parsed_sents(item)\n",
        "      for tree in trees:\n",
        "        tree = tree.leaves()\n",
        "        try:\n",
        "          PCFG_grammar.check_coverage(tree)\n",
        "\n",
        "          # Change the parsed tree from a tree to a string \n",
        "          # Use regular expression to find the correct chunk\n",
        "          # Delete the last character and then append to the all_p list\n",
        "          for parse in parser.parse(tree):\n",
        "            parse_string = str(parse)\n",
        "            p = re.search(r\"p=([^/]+)\", parse_string).group(1)\n",
        "            p = p[:-1]\n",
        "            all_p.append(float(p))\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "  return all_p\n",
        "\n",
        "\n",
        "# get the probabilities of our test trees\n",
        "all_p = all_parse_probabilities(grammar)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3zrraA1AmH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839123b4-014e-4097-e4dc-4e66a7ac8a09"
      },
      "source": [
        "def perplexity(all_p): \n",
        "\n",
        "    '''\n",
        "    Given a list of the probabilities of all parses from the testing set,\n",
        "    this calculates the perplexity of the model.\n",
        "    '''\n",
        "  \n",
        "    perplexity = 1\n",
        "\n",
        "    # N is the total number of probabilities \n",
        "    N = float(len(all_p))\n",
        "    for p in all_p:\n",
        "        # ignore infinity cases\n",
        "        if perplexity * (1/p) == float(\"inf\"):\n",
        "            continue\n",
        "        perplexity = perplexity * (1/p)\n",
        "    perplexity = pow(perplexity, 1/float(N))\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "# calculate perplexity for our test set\n",
        "print(\"Model perplexity on test set:\", perplexity(all_p))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model perplexity on test set: 5.614793240381743e+45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8gc3BrJVeMM"
      },
      "source": [
        "The model perplexity on our small test set comes out to be about 5e+45. Unfortunately, this is not an accurate representation of the PCFG model's comparison to the trigram model, since the train/test sets had to be reduced. But from this evaluation, the PCFG model appears to perform worse than the trigram model, since its perplexity is much higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAOm6eMDBiIR"
      },
      "source": [
        "## References\r\n",
        "\r\n",
        "\r\n",
        "*   https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\r\n",
        "*   https://nlpforhackers.io/language-models/\r\n",
        "* https://www.cs.bgu.ac.il/~elhadad/nlp16/NLTK-PCFG.html\r\n",
        "* https://www.asc.ohio-state.edu/demarneffe.1/LING5050/material/structured.html\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}